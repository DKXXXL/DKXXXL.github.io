---
layout: post
title:  "Notes on Automatic Differentiation, 0. Quick Intro."
date:   2019-09-28 00:56:11 -0400
categories: AD
---

Q: What is Automatic Differentiation?

A: Not rigiorously, consider a computer program as a function mapping between states, then this function in mathematical sense can have derivative. The motiviation is among the fields of Machine Learning and Numeral Analysis. 


Q: How to compute AD?

A: There are too many ways. A quick intro to find out one or two ways is in this [post](https://www.zhihu.com/question/66200879/answer/870023448)

Q: What is the complexity difference between Forward Mode and Reverse Mode?

A: The time complexity difference will become obvious only during computation of [tensor](https://math.stackexchange.com/questions/2195377/reverse-mode-differentiation-vs-forward-mode-differentiation-where-are-the-be). It basically become the question of "in which order to compute a consecutive multiplication of a sequence of matrices is the cheapest".


Q: What are all the possible ways of AD?

A: [This post](https://www.zhihu.com/question/358890800/answer/919289626) says classification includes Forward-Mode, Reverse-Mode, Source Transformation, Operator Overloading.

Q: Is operator overloading just dual number?

A: Seems like it according to this [wiki](https://en.wikipedia.org/wiki/Automatic_differentiation#Operator_overloading_(OO)). This method seems very easy to implememnt with forward-mode.  

Q: I can deal with dual number during source transformation, just like how easily Haskell can define a dual number [here](https://zhuanlan.zhihu.com/p/61521616). Then if operator overloading is just dual number, [how is a 'tape' needed](https://en.wikipedia.org/wiki/Automatic_differentiation#Operator_overloading_(OO))? And how is that not source transformation? 

A: ????

Q: How is operator overloading with reverse-mode implemented?

A: ????

